# Optimizing IoT Cloud Applications for Scalability: Leveraging RPC with Distributed Queues for Seamless Operations

Department of Computing, UFSCar - 2025

Author: Daniel Lombardi

Supervisor: Fredy João Valente

---

## ABSTRACT

The rapid expansion of the Internet of Things (IoT) has necessitated the development of robust and scalable cloud applications to manage the vast influx of data and ensure reliable communication between devices. This paper presents a comprehensive study backed by the practical experience gained during the development of large-scale networks with IoT actuators. We begin by exploring the unique challenges posed by IoT ecosystems, including real-time processing implementations and scaling for cloud applications and their difficulties. The study delves into the utilization of in-memory databases, queues and RPC methods to act as shared-memory, creating a vastly scalable distributed system that enables the utilization of traditional scaling methodologies for large MQTT based IoT environments.

## INTRODUCTION

The last decade has witnessed a revolution driven by the Internet of Things (IoT), transforming how we interact with the physical and digital worlds. The proliferation of connected devices, from environmental sensors to industrial actuators, has generated an unprecedented volume of data, opening new frontiers for automation, process optimization, and intelligent decision-making. (MOTTA, R. et al. 2019)

This exponential growth in connected devices and the ensuing data deluge inherently necessitate robust and scalable underlying infrastructure. Cloud computing has emerged as the indispensable backbone for IoT ecosystems, providing the computational power, storage capabilities, and network resources required to ingest, process, analyze, and act upon this vast influx of information. However, the sheer volume, velocity, and variety of data generated by millions, or even billions, of distributed IoT devices present significant challenges for traditional cloud application architectures. Ensuring real-time processing, maintaining low latency, and achieving horizontal scalability to accommodate dynamic workloads become critical factors for the successful deployment and operation of large-scale IoT solutions, especially when considering IoT actuators.

Unlike traditional IoT sensors, which primarily collect and transmit data, IoT actuators introduce a new layer of complexity to these distributed systems. These devices are designed to execute physical actions or trigger processes based on received commands, effectively closing the loop between the digital and physical worlds. Their integration demands not only robust data ingestion and processing capabilities but also rely on components where faulty connections are much more common than on traditional systems. This new paradigm of bidirectional communication and real-time control places unprecedented demands on the scalability, reliability, and architectural design of cloud-based IoT applications, often extending beyond the capabilities of pre-existing system designs focused solely on data collection.

Given the already specific demands of IoT applications, we must also consider the heterogeneity of tasks to be executed. Depending on the type of IoT device and it's function, the data and processing volume can be highly heterogeneous throughout the day. For instance, a smart lighting system might have minimal activity during daylight hours but experience peak command and telemetry traffic at dusk and dawn. Similarly, industrial sensors might burst data during specific operational cycles.

To effectively manage these fluctuating and diverse workloads, cloud computing providers rely on vast data center infrastructures with virtualized resources. They are made available to clients on a pay-as-you-go model, where customers pay according to their resource consumption, this is often offered as managed components such as Kubernetes clusters, load balancers and databases, constituting what is known as Infrastructure as a Service (IaaS). This model's inherent elasticity, particularly the horizontal scalability offered by platforms like Kubernetes, allows for the dynamic allocation and deallocation of resources in response to fluctuating demand, ensuring optimal performance during peak loads while minimizing costs during periods of low activity, making it ideal for modern IoT systems. (POURMAJIDI, W. et al. 2017).

Drawing from practical experience, our study examines a core problem arising from the implementation of an IoT solution in a hardware store, emphasizing the difficulties in achieving real-time processing and effective scaling in a cloud scenario. This problem became acutely evident during the development and deployment of Electronic Shelf Labels (ESLs) in a large-scale hardware store chain in Brazil. This particular implementation presented unique challenges, given the potential for deploying around 50 thousand ESLs per store, across dozens of stores nationwide, leading to a vast IoT ecosystem. We'll dive into the scaling limitations of traditional broker architectures as applied to our specific use case, particularly when managing the bidirectional communication and real-time control demands of large-scale Electronic Shelf Label (ESL) deployments. This includes a detailed examination of how the MQTT protocol, while effective for general load balancing, encounters significant hurdles in maintaining the request-reply pattern necessary for Remote Procedure Calls (RPCs) when horizontal scaling is implemented, as replies can be misrouted to different nodes than their originating requests. We propose and evaluate a novel approach leveraging distributed queues to overcome these architectural constraints, ensuring seamless, scalable, and reliable operations for IoT cloud applications involving actuators.

## THEORETICAL BASIS

This chapter will define the core concepts necessary to understand our solution, including the nuances of IoT ecosystems, the principles of cloud scalability, and the challenges of implementing Remote Procedure Calls (RPCs) over the MQTT protocol in horizontally scaled environments. It will further elaborate on how distributed queues and in-memory data stores are critical for overcoming these challenges, enabling seamless and reliable operations for large-scale IoT applications. This foundational knowledge is essential for appreciating the architectural design presented in this thesis. We will also do a quick analysis of used tools and technologies.

### The Internet of Thins (IoT) Ecosystem

As defined by Atzori et al., Internet of Things can be defined as three main paradigms, internet-oriented (middleware), things oriented (sensors) and semantic-oriented (knowledge). It is a network of physical objects embedded with technologies such as sensors, actuators and software that allows it to connect to external systems.

Sensors are devices that act as an intake point of data, they measure physical phenomena (i.e. temperature) and convert them to ditigal data. Actuators are devices that recieve commands from an external control system and converts them into physical actions (i.e. switching a light, changing a value etc.), they allow us to close the loop between the digital and physical worlds, commonly require bidirectional control as to acknowledge the commands. Our work focuses on the latter. Both sensors and actuators commonly operate via a GW (gateway) that act as a middleman from our servers to the devices (this allows us to have use different protocols such as bluetooth while still connecting them via other protocols to the servers, such as MQTT).

### Cloud Computing Fundamentals

Cloud computing, as defined by the National Institute of Standards and Technology (NIST), is a model for enabling ubiquitous, convenient, on-demand network access to a shared pool of configurable computing resources (e.g., networks, servers, storage, applications, and services) that can be rapidly provisioned and released with minimal management effort or service provider interaction. Essentially, it provides the on-demand availability of computer system resources, particularly data storage and computing power, without direct active management by the user. This paradigm has revolutionized how organizations deploy and manage their IT infrastructure, offering unprecedented flexibility, efficiency and security. Cloud services are offen offered as: Infrastructure as a Service (IaaS), Platform as a Service (Paas) and SaaS (System as a Service).

Having the flexibility and using PaaS to develop with Kubernetes truly unlocks powerful capabilities, especially when it comes to harnessing the core cloud benefit of elasticity. Elasticity in cloud computing refers to the ability of a system to rapidly and automatically adjust its computing resources (like CPU, memory, storage, network bandwidth and node count) to match fluctuating workloads and demands without human intervention. This dynamic scaling up or down ensures optimal performance and cost efficiency. This is particularly important when considering heterogeneous loads in IoT systems like the ESL described previously.

Scalability refers to a system's ability to receive more resources to increase its performance and capacity to handle demands. There are two basic types of scalability: vertical and horizontal (AAQIB, S, 2019).

Vertical Scaling (Scale-Up) is increasing the capacity of a single machine (i.e. adding more RAM, CPU). For truly large-scale systems, this is a great limitor when considering simple physical limits as how many RAM slots or CPU sockets a machine has. Furthermore, a single powerful machine represents a single point of failure, meaning any outage takes the entire system offline.

Horizontal Scaling (Scale-Out) is adding more machines to a computing cluster. This approach overcomes the limitations of vertical scaling by allowing for virtually limitless expansion. This not only provides superior fault tolerance – if one server fails, the others can continue operating – but also enables a "pay-as-you-grow" model, where resources are added incrementally as demand increases, leading to more efficient resource utilization and significant cost savings over time. Modern cloud architectures and technologies like Kubernetes are built fundamentally around horizontal scalability, enabling applications to dynamically scale out and in as demand fluctuates, ensuring high availability and optimal performance.

### Message Queues and Asynchronous Communication

Message Brokers (Queues) are essentially a kind of database that is optimized for handling message streams (GRAY, J. 1995). The Queue is the server, where producers and consumers connect to it as clients. Producers write messages to the broker, and the consumers recieve them. They allow easy communication between parties and act as a load balancer and buffer, they handle bursts of traffic by storing messages, preventing consumers from being overwhelmed. Queues store messages (tasks to be processed) until workers (consumers) are ready for them. This also means that consumers and producers can operate independently, without direct contact or knowledge of each other.

With the decoupling of producers and consumers, this means the tasks can be executed completely independent of when it was generated. This is called Asynchronous Communication. The producer only waits for the broker to confirm that it has buffered the message, not waiting for it to be executed. The delivery to consumers will happen at some undetermined future point in time—often within a fraction of a second, but sometimes significantly later if there is a queue backlog (KLEPPMANN, M. 2007).

The Advanced Message Queuing Protocol (AMQP) is an open standard application-layer protocol for message-oriented middleware. Unlike proprietary messaging systems, AMQP provides a vendor-neutral, interoperable framework for reliable, asynchronous communication between applications. It defines the mechanics for securely and efficiently transferring messages, often through a broker-based architecture where producers send messages to an intermediary (the broker), which then routes them to consumers. Key features include message orientation, queuing, flexible routing, and robust delivery guarantees (like at-least-once or at-most-once delivery), ensuring messages reach their intended recipients even if systems are temporarily unavailable.

Message Queuing Telemetry Transport (MQTT) is a lightweight, publish-subscribe messaging protocol designed for constrained devices and unreliable networks, making it ideal for the Internet of Things (IoT). Instead of devices communicating directly, they connect to a central MQTT broker. Devices that want to send data become publishers, sending messages to specific "topics" on the broker. Devices that want to receive data become subscribers, indicating interest in certain topics. The broker then efficiently delivers messages from publishers to all interested subscribers, decoupling senders from receivers and allowing for highly scalable and asynchronous communication. For load balancing, MQTT offers the round-robing algorithm for de,ivering messages inside listeners of a specific topic.

### Remote Procedure Call (RPC)

A Remote Procedure Call (RPC) is a proposal to allow programs to call procedures located on other machines, with the message passing part of it not being visible to the programmer. The main idea is that a process that may be complex is called externally from the main machine, allowing the programmer to not care about the address spaces, pameters and results marshalling. In the context of IoT, a common use case for an RPC is for when an actuator is called to execute a task. The client (programmers code) calls the server (IoT system) to toggle a light switch, and then replying if the task was successfull or not.

# TODO: REMOVE?

Challenges with Traditional RPC in Horizontally Scaled IoT: This is a crucial point from your introduction. Explain why directly implementing RPC over a simple pub/sub system like MQTT becomes problematic when horizontally scaled (replies being misrouted to different nodes than originating requests). This sets up the problem your solution addresses.

# TODO: Talvez remova isso aqui:

### In-Memory Data Stores / Databases

Purpose and Advantages:

Explain how in-memory databases (like Redis) store data primarily in RAM, offering extremely fast read and write operations.

Low Latency: Crucial for real-time IoT applications.

Shared State/Caching: How they can be used to manage shared state between distributed services (e.g., device configurations, session data, command statuses) or as a fast cache layer.

Dual Functionality (if applicable to your solution): If you use something like Redis, mention its versatility as both a cache/database and a message broker (e.g., Redis Streams, Pub/Sub, Lists).

### Kubernetes (and Container Orchestration)

Brief Overview:

Define Kubernetes as an open-source container orchestration system for automating deployment, scaling, and management of containerized applications.

Explain its role in providing the "Infrastructure as a Service" layer for your solution.
Key Features for Scalability: Mention concepts like Pods, Deployments, Services, Horizontal Pod Autoscaler (HPA), which enable dynamic scaling and load balancing of your application components.

By thoroughly explaining these concepts, you will prepare your readers to understand the problem you're addressing and the novelty and effectiveness of your proposed solution that leverages these components. Make sure to cite relevant academic sources for each concept where appropriate (e.g., for MQTT, cloud computing paradigms, distributed systems principles).
