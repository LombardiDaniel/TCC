# Optimizing IoT Cloud Applications for Scalability: Leveraging RPC with Distributed Queues for Seamless Operations

Department of Computing, UFSCar - 2025

Author: Daniel Lombardi

Supervisor: Fredy João Valente

---

## ABSTRACT

The rapid expansion of the Internet of Things (IoT) has necessitated the development of robust and scalable cloud applications to manage the vast influx of data and ensure reliable communication between devices. This paper presents a comprehensive study backed by the practical experience gained during the development of large-scale networks with IoT actuators. We begin by exploring the unique challenges posed by IoT ecosystems, including real-time processing implementations and scaling for cloud applications and their difficulties. The study delves into the utilization of in-memory databases, queues and RPC methods to act as shared-memory, creating a vastly scalable distributed system that enables the utilization of traditional scaling methodologies for large MQTT based IoT environments.

## INTRODUCTION

The last decade has witnessed a revolution driven by the Internet of Things (IoT), transforming how we interact with the physical and digital worlds. The proliferation of connected devices, from environmental sensors to industrial actuators, has generated an unprecedented volume of data, opening new frontiers for automation, process optimization, and intelligent decision-making. (MOTTA, R. et al. 2019)

This exponential growth in connected devices and the ensuing data deluge inherently necessitate robust and scalable underlying infrastructure. Cloud computing has emerged as the indispensable backbone for IoT ecosystems, providing the computational power, storage capabilities, and network resources required to ingest, process, analyze, and act upon this vast influx of information. However, the sheer volume, velocity, and variety of data generated by millions, or even billions, of distributed IoT devices present significant challenges for traditional cloud application architectures. Ensuring real-time processing, maintaining low latency, and achieving horizontal scalability to accommodate dynamic workloads become critical factors for the successful deployment and operation of large-scale IoT solutions, especially when considering IoT actuators.

Unlike traditional IoT sensors, which primarily collect and transmit data, IoT actuators introduce a new layer of complexity to these distributed systems. These devices are designed to execute physical actions or trigger processes based on received commands, effectively closing the loop between the digital and physical worlds. Their integration demands not only robust data ingestion and processing capabilities but also rely on components where faulty connections are much more common than on traditional systems. This new paradigm of bidirectional communication and real-time control places unprecedented demands on the scalability, reliability, and architectural design of cloud-based IoT applications, often extending beyond the capabilities of pre-existing system designs focused solely on data collection.

Given the already specific demands of IoT applications, we must also consider the heterogeneity of tasks to be executed. Depending on the type of IoT device and it's function, the data and processing volume can be highly heterogeneous throughout the day. For instance, a smart lighting system might have minimal activity during daylight hours but experience peak command and telemetry traffic at dusk and dawn. Similarly, industrial sensors might burst data during specific operational cycles.

To effectively manage these fluctuating and diverse workloads, cloud computing providers rely on vast data center infrastructures with virtualized resources. They are made available to clients on a pay-as-you-go model, where customers pay according to their resource consumption, this is often offered as managed components such as Kubernetes clusters, load balancers and databases, constituting what is known as Infrastructure as a Service (IaaS). This model's inherent elasticity, particularly the horizontal scalability offered by platforms like Kubernetes, allows for the dynamic allocation and deallocation of resources in response to fluctuating demand, ensuring optimal performance during peak loads while minimizing costs during periods of low activity, making it ideal for modern IoT systems. (POURMAJIDI, W. et al. 2017).

Drawing from practical experience, our study examines a core problem arising from the implementation of an IoT solution in a hardware store, emphasizing the difficulties in achieving real-time processing and effective scaling in a cloud scenario. This problem became acutely evident during the development and deployment of Electronic Shelf Labels (ESLs) in a large-scale hardware store chain in Brazil. This particular implementation presented unique challenges, given the potential for deploying around 50 thousand ESLs per store, across dozens of stores nationwide, leading to a vast IoT ecosystem. We'll dive into the scaling limitations of traditional broker architectures as applied to our specific use case, particularly when managing the bidirectional communication and real-time control demands of large-scale Electronic Shelf Label (ESL) deployments. This includes a detailed examination of how the MQTT protocol, while effective for general load balancing, encounters significant hurdles in maintaining the request-reply pattern necessary for Remote Procedure Calls (RPCs) when horizontal scaling is implemented, as replies can be misrouted to different nodes than their originating requests. We propose and evaluate a novel approach leveraging distributed queues to overcome these architectural constraints, ensuring seamless, scalable, and reliable operations for IoT cloud applications involving actuators.

## THEORETICAL BASIS

This chapter will define the core concepts necessary to understand our solution, including the nuances of IoT ecosystems, the principles of cloud scalability, and the challenges of implementing Remote Procedure Calls (RPCs) over the MQTT protocol in horizontally scaled environments. It will further elaborate on how distributed queues and in-memory data stores are critical for overcoming these challenges, enabling seamless and reliable operations for large-scale IoT applications. This foundational knowledge is essential for appreciating the architectural design presented in this thesis. We will also do a quick analysis of used tools and technologies.

### The Internet of Thins (IoT) Ecosystem

As defined by Atzori et al., Internet of Things can be defined as three main paradigms, internet-oriented (middleware), things oriented (sensors) and semantic-oriented (knowledge). It is a network of physical objects embedded with technologies such as sensors, actuators and software that allows it to connect to external systems.

Sensors are devices that act as an intake point of data, they measure physical phenomena (i.e. temperature) and convert them to digital data. Actuators are devices that receive commands from an external control system and converts them into physical actions (i.e. switching a light, changing a value etc.), they allow us to close the loop between the digital and physical worlds, commonly requiring bidirectional control to acknowledge the commands. Our work focuses on the latter. Both sensors and actuators commonly operate via a GW (gateway) that act as a middleman from our servers to the devices (this allows us to have use different protocols such as bluetooth while still connecting them via other protocols to the servers, such as MQTT).

### Scalability

Scalability is a paramount concern for any robust system, especially in the context of dynamic environments like the Internet of Things (IoT). As client loads increase, whether from a surge in active users, a higher volume of data processing, or more frequent device interaction, a system must demonstrate its ability to cope with these elevated demands. This is not a binary state, but rather a spectrum defined by how effectively computing resources can be added to maintain performance as the "load parameters" (e.g., requests per second, read-to-write ratios, number of concurrent connections) evolve. The challenge often lies not in handling individual operations, but in managing the "fan-out" effect, as seen in the famous Twitter Timeline (KRIKORIAN R. 2012) example where a single tweet from a celebrity needs to be delivered to millions of followers, creating a vastly multiplied write burden, requiring special case for some users that would appen non-ideal in other scenarios. Transforming the multiple write burden in simple cache reads, combining the two methods of writing to each user's timeline and having dedicated caches for users with a large number of followers.

To address these escalating demands, systems typically employ two fundamental scaling strategies: vertical and horizontal. Vertical scaling, or "scaling up," involves enhancing the capabilities of a single machine by adding more CPU, RAM, or storage. While straightforward, this approach is severely limited by the physical constraints of hardware and the exponential cost associated with high-end, specialized servers. Crucially, it also introduces a single point of failure, meaning that the entire system's availability hinges on that one machine, which is unacceptable for large-scale, mission-critical applications where continuous operation is paramount.

Conversely, horizontal scaling, or "scaling out," offers a more adaptable and resilient solution by distributing the workload across multiple machines within a cluster. This method provides virtually limitless expansion potential; as load increases, more commodity servers can be added to the pool, and a load balancer can distribute incoming requests among them. This not only inherently builds in fault tolerance—the failure of one node does not incapacitate the entire system—but also aligns perfectly with the "pay-as-you-grow" model prevalent in cloud computing. Technologies like Kubernetes exemplify this, dynamically adjusting the number of running application instances (pods) and even the underlying infrastructure nodes to match fluctuating demand, thereby optimizing resource utilization and ensuring continuous performance while managing costs effectively.

However, the advantages of horizontal scaling come with their own set of architectural complexities. Ensuring data consistency across numerous distributed nodes, especially for stateful applications, requires careful design and the implementation of sophisticated mechanisms like distributed queues (e.g., RabbitMQ, Apache Kafka), which manage message streams asynchronously and reliably even if individual components fail. Furthermore, the operational overhead of managing a large, distributed cluster necessitates advanced tools for monitoring, deployment, and orchestration, while developers must design "cloud-native" applications that are inherently suited to a distributed, stateless, or externally-state-managed environment to truly leverage horizontal scalability.

### Cloud Computing Fundamentals

Cloud computing, as defined by the National Institute of Standards and Technology (NIST), is a model for enabling ubiquitous, convenient, on-demand network access to a shared pool of configurable computing resources (e.g., networks, servers, storage, applications, and services) that can be rapidly provisioned and released with minimal management effort or service provider interaction. Essentially, it provides the on-demand availability of computer system resources, particularly data storage and computing power, without direct active management by the user. This paradigm has revolutionized how organizations deploy and manage their IT infrastructure, offering unprecedented flexibility, efficiency and security. Cloud services are offen offered as: Infrastructure as a Service (IaaS), Platform as a Service (Paas) and SaaS (System as a Service).

Having the flexibility and using PaaS to develop with Kubernetes truly unlocks powerful capabilities, especially when it comes to harnessing the core cloud benefit of elasticity. Elasticity in cloud computing refers to the ability of a system to rapidly and automatically adjust its computing resources (like CPU, memory, storage, network bandwidth and node count) to match fluctuating workloads and demands without human intervention. This dynamic scaling up or down ensures optimal performance and cost efficiency. This is particularly important when considering heterogeneous loads in IoT systems like the ESL described previously.

Scalability refers to a system's ability to receive more resources to increase its performance and capacity to handle demands. There are two basic types of scalability: vertical and horizontal (AAQIB, S, 2019).

Vertical Scaling (Scale-Up) is increasing the capacity of a single machine (i.e. adding more RAM, CPU). For truly large-scale systems, this is a great limiter when considering simple physical limits as to how many RAM slots or CPU sockets a machine has. Furthermore, a single powerful machine represents a single point of failure, meaning any outage takes the entire system offline.

Horizontal Scaling (Scale-Out) is adding more machines to a computing cluster. This approach overcomes the limitations of vertical scaling by allowing for virtually limitless expansion. This not only provides superior fault tolerance – if one server fails, the others can continue operating – but also enables a "pay-as-you-grow" model, where resources are added incrementally as demand increases, leading to more efficient resource utilization and significant cost savings over time. Modern cloud architectures and technologies like Kubernetes are built fundamentally around horizontal scalability, enabling applications to dynamically scale out and in as demand fluctuates, ensuring high availability and optimal performance.

### Message Queues and Asynchronous Communication

Message Brokers (Queues) are essentially a kind of database that is optimized for handling message streams (GRAY, J. 1995). The Queue is the server, where producers and consumers connect to it as clients. Producers write messages to the broker, and the consumers recieve them. They allow easy communication between parties and act as a load balancer and buffer, they handle bursts of traffic by storing messages, preventing consumers from being overwhelmed. Queues store messages (tasks to be processed) until workers (consumers) are ready for them in a First-In, First-Out (FIFO) matter. This also means that consumers and producers can operate independently, without direct contact or knowledge of each other.

With the decoupling of producers and consumers, this means the tasks can be executed completely independent of when it was generated. This is called Asynchronous Communication. The producer only waits for the broker to confirm that it has buffered the message, not waiting for it to be executed. The delivery to consumers will happen at some undetermined future point in time—often within a fraction of a second, but sometimes significantly later if there is a queue backlog (KLEPPMANN, M. 2007).

The Advanced Message Queuing Protocol (AMQP) is an open standard application-layer protocol for message-oriented middleware. Unlike proprietary messaging systems, AMQP provides a vendor-neutral, interoperable framework for reliable, asynchronous communication between applications. It defines the mechanics for securely and efficiently transferring messages, often through a broker-based architecture where producers send messages to an intermediary (the broker), which then routes them to consumers. Key features include message orientation, queuing, flexible routing, and robust delivery guarantees (like at-least-once or at-most-once delivery), ensuring messages reach their intended recipients even if systems are temporarily unavailable.

Message Queuing Telemetry Transport (MQTT) is a lightweight, publish-subscribe messaging protocol designed for constrained devices and unreliable networks, making it ideal for the Internet of Things (IoT). Instead of devices communicating directly, they connect to a central MQTT broker. Devices that want to send data become publishers, sending messages to specific "topics" on the broker. Devices that want to receive data become subscribers, indicating interest in certain topics. The broker then efficiently delivers messages from publishers to all interested subscribers, decoupling senders from receivers and allowing for highly scalable and asynchronous communication. For load balancing, MQTT offers the round-robin algorithm for delivering messages inside listeners of a specific topic.

![queue](/thesis/static/queue.png)

### Remote Procedure Call (RPC)

A Remote Procedure Call (RPC) is a proposal to allow programs to call procedures located on other machines, with the message passing part of it not being visible to the programmer. The main idea is that a process that may be complex is called externally from the main machine, allowing the programmer to not care about the address spaces, pameters and results marshalling. In the context of IoT, a common use case for an RPC is for when an actuator is called to execute a task. The client (programmers code) calls the server (IoT system) to toggle a light switch, and then replying if the task was successful or not.

![rpc](/thesis/static/rpc.png)

### In-Memory Data Stores / Databases

In-memory datastores like Redis play a crucial role in modern distributed systems, particularly within the IoT ecosystem, by offering extremely fast data access and manipulation. Unlike traditional disk-based databases, Redis stores its data set primarily in RAM, significantly reducing latency for read and write operations, which is critical for real-time applications and rapidly changing data that doesn't require long-term persistence. This allows for rapid retrieval and updates of frequently accessed, volatile information, directly enhancing the responsiveness and efficiency of control systems or RPC calls by avoiding slower disk I/O. Its ability to handle high-throughput, low-latency operations for temporary data makes in-memory datastores invaluable for managing the vast and often dynamic information generated by IoT devices. Redis in particular stores data in a key-value storage model, a non-relational database that stores data as a collection of unique keys, each paired with an associated value, offering a simple yet highly efficient model for data retrieval. The inherent simplicity of the key-value model, coupled with Redis's in-memory architecture, makes it highly amenable to horizontal scalability through sharding or clustering, where the dataset is partitioned and distributed across multiple Redis instances. This allows for near-linear scaling of throughput and capacity by simply adding more nodes, ensuring that as the volume of ephemeral data or the number of read/write operations grows, the system can expand to meet demand without hitting the limitations of a single machine.

### Kubernetes (and Container Orchestration)

Kubernetes is an open-source container orchestration platform that inherently delivers high scalability and elasticity for applications. It achieves this by managing containerized workloads and services, abstracting away the underlying infrastructure. Its core features, like the Horizontal Pod Autoscaler (HPA), automatically adjust the number of running application instances (pods) based on metrics such as CPU utilization or custom application-specific demands, ensuring that enough capacity is always available to handle fluctuating user traffic without manual intervention. Complementing this, the Cluster Autoscaler works at a lower level, dynamically adding or removing worker nodes (the virtual machines that run the pods) from the underlying cloud provider's infrastructure. This intelligent, automated scaling ensures optimal resource utilization, as the system can rapidly expand during peak loads and contract during off-peak periods, directly translating to cost efficiency by only paying for the resources actively consumed, making it ideal for the dynamic and often unpredictable workloads found in large-scale IoT systems.

## METHODOLOGY

Considering the importance and challenges of achieving true scalability in large distributed IoT systems, this chapter describes the development of a new method of communication for RPC for IoT Cloud Applications, one that leverages the elasticity of cloud environments with kubernetes to achieve true horizontal scalability in an MQTT IoT actuator network. This will be called our Communication Backbone.

On top of that, we also discuss methodologies to test, measure and validate the proposed system, as well as comparing it to the de facto method for such cases, both in scalability as well as complexity of configuration and Time-to-Deploy/Onboard new devices.

### General Architecture

The Communication Backbone developed for this thesis consists of several components:

![simplified architecture](/thesis/static/simplified_arch.png)

#### Task Queue

Firstly, the `task_queue` is responsible for delivering tasks to our `task_worker`. Tasks are actions to be executed in the IoT devices (ESLs, in our case). The utilization of a queue here is paramount, primarily because these actions can be inherently long-running, often with an estimated timeout of around three minutes per task.

This potential for delays makes a synchronous request-reply model impractial and innefficient for the initial command dispatch. If the calling application were to wait synchronously for three minutes for each ESL action to complete, its threads would be tied up, quickly exhausting resources and severely limiting the system's overall throughput and responsiveness. This is particularly problematic in a large-scale IoT environment where thousands or tens of thousands of simultaneous commands might be issued (e.g., updating prices across an entire store).

By leveraging asynchronous communication via the task_queue, the producer (application that dispatches the tasks) can immediately hand off the task to the queue and consider its part of the transaction complete. It does not need to block and wait for the ESL to process the command. The queue then acts as a durable buffer, ensuring that the task is persisted and will eventually be delivered to an available task_worker, even if the worker or the IoT device experiences temporary disconnections or processing delays. This decoupling allows the producer to quickly move on to process other commands or serve other users, dramatically improving the perceived responsiveness and overall system capacity. It also provides inherent resilience: if a task_worker fails, the tasks remain in the queue to be picked up by another worker once available, preventing data loss and ensuring eventual consistency.

Furthermore, a key advantage of using a robust message queue is its ability to guarantee specific delivery semantics. In our model, this is extremely important, as we want to guarantee at-least-once execution semantics for critical operations like price updates. This means that once a price update command is sent to the task_queue, the system ensures it will be processed and executed by a task_worker at least one time. This guarantee is vital for business-critical operations like price synchronization, as it prevents scenarios where a price update might be lost due to transient network issues, worker failures, or device unresponsiveness. The queue achieves this through mechanisms such as message acknowledgments: a message is only considered processed and removed from the queue after the task_worker explicitly acknowledges its successful handling, or after a configurable timeout, upon which it can be redelivered to another worker (KLEPPMANN, M. 2017). This robustness is non-negotiable for ensuring that all prices displayed on ESLs are consistently and accurately updated, directly addressing a core business requirement.

#### Business Layer (Task Worker)

The task_worker is responsible for consuming messages (tasks) from the task_queue. Crucially, the task_worker encapsulates the core business logic associated with processing these commands. This includes interacting with databases (e.g., to fetch detailed product information for a price update, log the command's status, and update the device's state) and any other external services required to fulfill the task. Upon receiving a task and initial checks and processing is completed, the task_worker makes an RPC request to the Communication Layer of our system.

#### Communication Layer (Router)

The Communication Layer (Router) is responsible for everything regarding the addressing and messaging the individual devices. It connects to the subsequest MQTT Layer and holds the Communication Backbone logic. It allows the task_worker to be completely technology and vendor independant (when referring to the IoT devices).

Since the designed IoT devices communicate in Bluetooth Low Energy (BLE), there are intermediate Gateway Devices to "translate" requests to BLE. To reach a target device (ESL), the intermediate gateway hop also needs to be addressed, as such, the Communcation Layer is also responsible for identifying the correct gateway that the ESL is currently addressable to. This identification is dynamic and crucial for reliable communication; for any given ESL, there might be multiple gateways within its BLE range. The Communication Layer must intelligently determine the optimal gateway to route the request through, often by selecting the gateway reporting the strongest Received Signal Strength Indicator (RSSI) from the ESL. This ensures the command is sent via the most stable and reliable path, minimizing signal loss and improving command success rates. This also saves us from needing more complex methods for reaching BLE signal stability, wich would be too complex considering the dynamic environment inside the construction stores (THALJAOUI, A. et al. 2015).

This Communication Layer inherently represents the primary scalability challenge for the entire system. While the task_queue and task_workers can scale horizontally with relative ease by adding more instances, the Communication Layer faces unique challenges in maintaining device connectivity and command routing efficiency across a massive and dynamic network of IoT gateways and devices. In traditional architectures, the load-balancing done by MQTT would not allow a system to simply scale horizontally, as the nodes that recieve the RPC request may not be the same ones that recieve the response from the device. This "misrouting" of replies would lead to responses being discarded as unrelated or "junk messages," effectively breaking the critical request-reply pattern essential for reliable actuator control and feedback. Our Communication Layer's specialized logic directly addresses this limitation by ensuring response correlation and proper routing, which is fundamental to overcoming this scalability hurdle.

Any bottleneck or inefficiency within this intricate layer can propagate throughout the entire system, severely hindering the overall scalability, responsiveness, and reliability of the large-scale IoT application.
